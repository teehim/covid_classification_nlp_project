{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DEMO_Thai_dataset_Fine_Tuning BERT for  sentiment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9lh-o__dZxz"
      },
      "source": [
        "# !pip install transformers==3.5.1\n",
        "# !pip install torch==1.7.1"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4giRzM7NtHJ"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import transformers\n",
        "from transformers import AutoModel, BertTokenizerFast, CamembertTokenizer\n",
        "\n",
        "# specify GPU\n",
        "device = torch.device(\"cuda\")"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QN93FLH3YMF8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2be99755-9c2f-4ad6-aadd-2bd266593b61"
      },
      "source": [
        "# For google colab\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKd-Tj3hOMsZ"
      },
      "source": [
        "# Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwJrQFQgN_BE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "ee630492-a3a5-4385-d5d9-e4864d30e816"
      },
      "source": [
        "# Binary classification dataset\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/7204/Lab3/BERT/cold_code.csv\",)\n",
        "\n",
        "# Multiclass classification dataset\n",
        "# df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/7204/Lab3/BERT/cold_multi.csv\",)\n",
        "\n",
        "df.head()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>ภูมิแพ้ชนิดนี้จะมีอาการที่เกิดกับจมูก บริเวณโพ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>มีอาการตั้งแต่น้ำมูกไหล จาม คันจมูก คัดจมูก (ค...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>อาการจามบ่อยๆ น้ำมูกใส คัดจมูก อาจเป็นช่วงใดช่...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>มีน้ำมูกไหล ในลักษณะที่ใส , คันจมูก , จาม หรือ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>อาการคัดจมูก จาม มีน้ำมูกใสๆ คันจมูก ถ้าเป็นโร...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   label                                               text\n",
              "0      0  ภูมิแพ้ชนิดนี้จะมีอาการที่เกิดกับจมูก บริเวณโพ...\n",
              "1      0  มีอาการตั้งแต่น้ำมูกไหล จาม คันจมูก คัดจมูก (ค...\n",
              "2      0  อาการจามบ่อยๆ น้ำมูกใส คัดจมูก อาจเป็นช่วงใดช่...\n",
              "3      0  มีน้ำมูกไหล ในลักษณะที่ใส , คันจมูก , จาม หรือ...\n",
              "4      0  อาการคัดจมูก จาม มีน้ำมูกใสๆ คันจมูก ถ้าเป็นโร..."
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKfWnApvOoE7"
      },
      "source": [
        "# Split train dataset into train, validation and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfhSPF5jOWb7"
      },
      "source": [
        "train_text, temp_text, train_labels, temp_labels = train_test_split(df['text'], df['label'], \n",
        "                                                                    random_state=2018, \n",
        "                                                                    test_size=0.7, \n",
        "                                                                    stratify=df['label'])\n",
        "\n",
        "# we will use temp_text and temp_labels to create validation and test set\n",
        "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
        "                                                                random_state=2018, \n",
        "                                                                test_size=0.5, \n",
        "                                                                stratify=temp_labels)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7hsdLoCO7uB"
      },
      "source": [
        "# Import BERT Model and BERT Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1kY3gZjO2RE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b2eaca1-0855-451a-a9a4-edb36df3d4b5"
      },
      "source": [
        "model_name = \"Geotrend/bert-base-th-cased\"\n",
        "\n",
        "# import BERT-base pretrained model\n",
        "bert = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained(model_name)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertModel were not initialized from the model checkpoint at Geotrend/bert-base-th-cased and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaY6gG0VNHaC"
      },
      "source": [
        "# # For wangchanberta\n",
        "# model_name = \"airesearch/wangchanberta-base-att-spm-uncased\"\n",
        "\n",
        "# # import BERT-base pretrained model\n",
        "# bert = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "# # Load the BERT tokenizer\n",
        "# tokenizer = CamembertTokenizer.from_pretrained(model_name)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wIYaWI_Prg8"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKwbpeN_PMiu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "ab7529bf-cb11-4b21-9f66-87c98fe2d011"
      },
      "source": [
        "# get length of all the messages in the train set\n",
        "seq_len = [len(i.split()) for i in train_text]\n",
        "\n",
        "pd.Series(seq_len).hist(bins = 30)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f9b069d6f50>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAU50lEQVR4nO3db6xcd53f8fcX20DEZZ3uJrobOaamJWqF4t2Ab5NUrKq5QVQmiZJdNdsGZVm8ApmuSDdbhRaHB6FEQhukhpRdEMglaQJNuUEBiptktRsR3w08IGAHEycxtGbrqvamNonB4UI2Ky/fPpjjdjyeuXPmz50/P71f0shz5vzOmc/8NP7k5HjmTGQmkqTZ96pJB5AkjYaFLkmFsNAlqRAWuiQVwkKXpEKsn9QTX3DBBblly5aBtv3Zz37G6173utEGWmNmHo9ZyzxrecHM49It8/79+1/IzAs7bpSZE7lt27YtB7V3796Bt50UM4/HrGWetbyZZh6XbpmBfdmlVz3lIkmFsNAlqRAWuiQVwkKXpEJY6JJUCAtdkgpRu9AjYl1EfDciHu6w7jUR8WBEHI6IJyNiyyhDSpJ66+cI/RbgUJd17wV+nJlvAu4GPj5sMElSf2oVekRcDFwDfK7LkOuB+6v7DwFvj4gYPp4kqa7IGj9wEREPAX8EvB74YGZe27b+GWB7Zh6tln8IXJGZL7SN2wnsBJifn9+2tLQ0UOiVlRXm5uYG2nZYB4+dqjVu66aNZy1PMvOgzLz2Zi0vmHlcumVeXFzcn5kLnbbpeS2XiLgWOJGZ+yOiMUzAzNwN7AZYWFjIRmOw3S0vLzPotsPaseuRWuOO3NQ4a3mSmQdl5rU3a3nBzOMySOY6p1zeBlwXEUeAJeCqiPjPbWOOAZsBImI9sBF4sa8kkqSh9Cz0zLwtMy/OzC3AjcDjmfk7bcP2AO+p7t9QjfHHSiVpjAa+fG5E3EHzql97gHuAL0TEYeAkzeKXJI1RX4WemcvAcnX/9pbH/xr47VEGkyT1x2+KSlIhLHRJKoSFLkmFsNAlqRAWuiQVwkKXpEJY6JJUCAtdkgphoUtSISx0SSqEhS5JhbDQJakQFrokFcJCl6RCWOiSVAgLXZIK0bPQI+K1EfHtiPheRDwbER/tMGZHRPwoIg5Ut/etTVxJUjd1frHoFeCqzFyJiA3ANyPiTzPzW23jHszMm0cfUZJUR89Cr37seaVa3FDd/AFoSZoytc6hR8S6iDgAnAAey8wnOwz7ZxHxdEQ8FBGbR5pSktRTNA/Aaw6OOB/4KvCvMvOZlsd/BVjJzFci4v3Av8jMqzpsvxPYCTA/P79taWlpoNArKyvMzc0NtO2wDh47VWvc1k0bz1qeZOZBmXntzVpeMPO4dMu8uLi4PzMXOm3TV6EDRMTtwM8z8993Wb8OOJmZGzutP2NhYSH37dvX13Ofsby8TKPRGGjbYW3Z9UitcUfuvOas5UlmHpSZ196s5QUzj0u3zBHRtdDrfMrlwurInIg4D3gH8P22MRe1LF4HHKofW5I0CnU+5XIRcH915P0q4EuZ+XBE3AHsy8w9wB9ExHXAaeAksGOtAkuSOqvzKZengbd0ePz2lvu3AbeNNpokqR9+U1SSCmGhS1IhLHRJKoSFLkmFsNAlqRAWuiQVwkKXpEJY6JJUCAtdkgphoUtSISx0SSqEhS5JhbDQJakQFrokFcJCl6RCWOiSVAgLXZIKUec3RV8bEd+OiO9FxLMR8dEOY14TEQ9GxOGIeDIitqxFWElSd3WO0F8BrsrMXwcuA7ZHxJVtY94L/Dgz3wTcDXx8tDElSb30LPRsWqkWN1S3bBt2PXB/df8h4O0RESNLKUnqKTLbu7nDoIh1wH7gTcCnM/NDbeufAbZn5tFq+YfAFZn5Qtu4ncBOgPn5+W1LS0sDhV5ZWWFubm6gbYd18NipWuO2btp41vKwmes+bz/aM7ab5DwPatYyz1peMPO4dMu8uLi4PzMXOm2zvs6OM/Nvgcsi4nzgqxFxaWY+02/AzNwN7AZYWFjIRqPR7y4AWF5eZtBth7Vj1yO1xh25qXHW8rCZ6z5vP9oztpvkPA9q1jLPWl4w87gMkrmvT7lk5k+AvcD2tlXHgM0AEbEe2Ai82FcSSdJQ6nzK5cLqyJyIOA94B/D9tmF7gPdU928AHs8653IkSSNT55TLRcD91Xn0VwFfysyHI+IOYF9m7gHuAb4QEYeBk8CNa5ZYktRRz0LPzKeBt3R4/PaW+38N/PZoo0mS+uE3RSWpEBa6JBXCQpekQljoklQIC12SCmGhS1IhLHRJKoSFLkmFsNAlqRAWuiQVwkKXpEJY6JJUCAtdkgphoUtSISx0SSqEhS5JhbDQJakQdX5TdHNE7I2I5yLi2Yi4pcOYRkSciogD1e32TvuSJK2dOr8pehq4NTOfiojXA/sj4rHMfK5t3Dcy89rRR5Qk1dHzCD0zn8/Mp6r7PwUOAZvWOpgkqT+RmfUHR2wBngAuzcyXWh5vAF8GjgJ/BXwwM5/tsP1OYCfA/Pz8tqWlpYFCr6ysMDc3N9C2wzp47FStcVs3bTxredjMdZ+3H+0Z201yngc1a5lnLS+YeVy6ZV5cXNyfmQudtqld6BExB/wF8LHM/Erbul8CfpGZKxFxNfDJzLxktf0tLCzkvn37aj13u+XlZRqNxkDbDmvLrkdqjTty5zVnLQ+bue7z9qM9Y7tJzvOgZi3zrOUFM49Lt8wR0bXQa33KJSI20DwCf6C9zAEy86XMXKnuPwpsiIgL+sguSRpSnU+5BHAPcCgzP9FlzK9W44iIy6v9vjjKoJKk1dX5lMvbgHcDByPiQPXYh4E3AGTmZ4EbgN+PiNPAy8CN2c/JeUnS0HoWemZ+E4geYz4FfGpUoSRJ/fObopJUCAtdkgphoUtSISx0SSqEhS5JhbDQJakQFrokFcJCl6RCWOiSVAgLXZIKYaFLUiEsdEkqhIUuSYWw0CWpEBa6JBXCQpekQljoklSIOr8pujki9kbEcxHxbETc0mFMRMQfR8ThiHg6It66NnElSd3U+U3R08CtmflURLwe2B8Rj2Xmcy1j3glcUt2uAD5T/SlJGpOeR+iZ+XxmPlXd/ylwCNjUNux64PPZ9C3g/Ii4aORpJUldRWbWHxyxBXgCuDQzX2p5/GHgzuoHpYmIrwMfysx9bdvvBHYCzM/Pb1taWhoo9MrKCnNzcwNtO6yDx07VGrd108azlrtlrru/SZg/D46/fO5r6WbQuRnl/ib53hjErOUFM49Lt8yLi4v7M3Oh0zZ1TrkAEBFzwJeBP2wt835k5m5gN8DCwkI2Go1BdsPy8jKDbjusHbseqTXuyE2Ns5a7Za67v0m4detp7jq4/pzX0s2gczPK/U3yvTGIWcsLZh6XQTLX+pRLRGygWeYPZOZXOgw5BmxuWb64ekySNCZ1PuUSwD3Aocz8RJdhe4DfrT7tciVwKjOfH2FOSVIPdU65vA14N3AwIg5Uj30YeANAZn4WeBS4GjgM/Bz4vdFHlSStpmehV//QGT3GJPCBUYWSJPXPb4pKUiEsdEkqhIUuSYWw0CWpEBa6JBXCQpekQljoklQIC12SCmGhS1IhLHRJKoSFLkmFsNAlqRAWuiQVwkKXpEJY6JJUCAtdkgphoUtSIer8pui9EXEiIp7psr4REaci4kB1u330MSVJvdT5TdH7gE8Bn19lzDcy89qRJJIkDaTnEXpmPgGcHEMWSdIQovn7zj0GRWwBHs7MSzusawBfBo4CfwV8MDOf7bKfncBOgPn5+W1LS0sDhV5ZWWFubm6gbYd18NipWuO2btp41nK3zHX3Nwnz58Hxl899Ld0MOjej3N8k3xuDmLW8YOZx6ZZ5cXFxf2YudNpmFIX+S8AvMnMlIq4GPpmZl/Ta58LCQu7bt6/nc3eyvLxMo9EYaNthbdn1SK1xR+685qzlbpnr7m8Sbt16mrsOrj/ntXQz6NyMcn+TfG8MYtbygpnHpVvmiOha6EN/yiUzX8rMler+o8CGiLhg2P1KkvozdKFHxK9GRFT3L6/2+eKw+5Uk9afnp1wi4otAA7ggIo4CHwE2AGTmZ4EbgN+PiNPAy8CNWec8jiRppHoWema+q8f6T9H8WKMkaYL8pqgkFcJCl6RCWOiSVAgLXZIKYaFLUiEsdEkqhIUuSYWw0CWpEBa6JBXCQpekQljoklQIC12SCmGhS1IhLHRJKoSFLkmFsNAlqRAWuiQVomehR8S9EXEiIp7psj4i4o8j4nBEPB0Rbx19TElSL3WO0O8Dtq+y/p3AJdVtJ/CZ4WNJkvrVs9Az8wng5CpDrgc+n03fAs6PiItGFVCSVE9kZu9BEVuAhzPz0g7rHgbuzMxvVstfBz6Umfs6jN1J8yie+fn5bUtLSwOFPnHyFMdfHmjTsdm6aeNZyysrK8zNzZ0z7uCxU+OK1Lf58+D4y+e+lm4m9Vpa83Wb52k1a3lh+jN3eh+eeS+3qvu+Hva5O6nz3N3meXFxcX9mLnTaZn2tZx+RzNwN7AZYWFjIRqMx0H7+5IGvcdfBsUbv25GbGmctLy8v0+n17tj1yHgCDeDWrae56+D6c15LN5N6La35us3ztJq1vDD9mTu9D8+8l1vVfV8P+9yd1HnuQeZ5FJ9yOQZsblm+uHpMkjRGoyj0PcDvVp92uRI4lZnPj2C/kqQ+9DxvERFfBBrABRFxFPgIsAEgMz8LPApcDRwGfg783lqFlSR117PQM/NdPdYn8IGRJZIkDcRvikpSISx0SSqEhS5JhbDQJakQFrokFcJCl6RCWOiSVAgLXZIKYaFLUiEsdEkqhIUuSYWw0CWpEBa6JBXCQpekQljoklQIC12SCmGhS1IhahV6RGyPiB9ExOGI2NVh/Y6I+FFEHKhu7xt9VEnSaur8pug64NPAO4CjwHciYk9mPtc29MHMvHkNMkqSaqhzhH45cDgz/zIz/wZYAq5f21iSpH5F8zeeVxkQcQOwPTPfVy2/G7ii9Wg8InYAfwT8CPjvwL/OzP/dYV87gZ0A8/Pz25aWlgYKfeLkKY6/PNCmY7N108azlldWVpibmztn3MFjp8YVqW/z58Hxl899Ld1M6rW05us2z9Nq1vLC9Gfu9D48815uVfd9Pexzd1LnubvN8+Li4v7MXOi0Tc9TLjX9N+CLmflKRLwfuB+4qn1QZu4GdgMsLCxko9EY6Mn+5IGvcdfBUUVfG0duapy1vLy8TKfXu2PXI+MJNIBbt57mroPrz3kt3UzqtbTm6zbP02rW8sL0Z+70PjzzXm5V93097HN3Uue5B5nnOqdcjgGbW5Yvrh77fzLzxcx8pVr8HLCtrxSSpKHVKfTvAJdExBsj4tXAjcCe1gERcVHL4nXAodFFlCTV0fO8RWaejoibgT8D1gH3ZuazEXEHsC8z9wB/EBHXAaeBk8CONcwsSeqg1onozHwUeLTtsdtb7t8G3DbaaJKkfvhNUUkqhIUuSYWw0CWpEBa6JBXCQpekQljoklQIC12SCmGhS1IhLHRJKoSFLkmFsNAlqRAWuiQVwkKXpEJY6JJUCAtdkgphoUtSISx0SSpErUKPiO0R8YOIOBwRuzqsf01EPFitfzIitow6qCRpdT0LPSLWAZ8G3gm8GXhXRLy5bdh7gR9n5puAu4GPjzqoJGl1dY7QLwcOZ+ZfZubfAEvA9W1jrgfur+4/BLw9ImJ0MSVJvURmrj4g4gZge2a+r1p+N3BFZt7cMuaZaszRavmH1ZgX2va1E9hZLf4D4AcD5r4AeKHnqOli5vGYtcyzlhfMPC7dMv/dzLyw0wbr1zbP2TJzN7B72P1ExL7MXBhBpLEx83jMWuZZywtmHpdBMtc55XIM2NyyfHH1WMcxEbEe2Ai82E8QSdJw6hT6d4BLIuKNEfFq4EZgT9uYPcB7qvs3AI9nr3M5kqSR6nnKJTNPR8TNwJ8B64B7M/PZiLgD2JeZe4B7gC9ExGHgJM3SX0tDn7aZADOPx6xlnrW8YOZx6Ttzz38UlSTNBr8pKkmFsNAlqRAzV+gRcSQiDkbEgYjYN+k8nUTEvRFxovp8/pnHfjkiHouI/1H9+XcmmbFdl8z/LiKOVXN9ICKunmTGVhGxOSL2RsRzEfFsRNxSPT6187xK5mme59dGxLcj4ntV5o9Wj7+xuszH4eqyH6+edFZYNe99EfE/W+b4sklnbRcR6yLiuxHxcLXc9xzPXKFXFjPzsin+XOl9wPa2x3YBX8/MS4CvV8vT5D7OzQxwdzXXl2Xmo2POtJrTwK2Z+WbgSuAD1SUppnmeu2WG6Z3nV4CrMvPXgcuA7RFxJc3Le9xdXe7jxzQv/zENuuUF+Dctc3xgchG7ugU41LLc9xzPaqFPtcx8guanfVq1Xh7hfuA3xxqqhy6Zp1ZmPp+ZT1X3f0rzL8ImpnieV8k8tbJppVrcUN0SuIrmZT5giuZ5lbxTLSIuBq4BPlctBwPM8SwWegJ/HhH7q0sJzIr5zHy+uv9/gPlJhunDzRHxdHVKZmpOX7Sqru75FuBJZmSe2zLDFM9zdSrgAHACeAz4IfCTzDxdDTnKFP2HqT1vZp6Z449Vc3x3RLxmghE7+Q/AvwV+US3/CgPM8SwW+m9k5ltpXv3xAxHxTyYdqF/Vl66m/qgB+Azw92n+r+vzwF2TjXOuiJgDvgz8YWa+1LpuWue5Q+apnufM/NvMvIzmt8QvB/7hhCOtqj1vRFwK3EYz9z8Cfhn40AQjniUirgVOZOb+Yfc1c4WemceqP08AX6X5BpsFxyPiIoDqzxMTztNTZh6v/nL8AviPTNlcR8QGmsX4QGZ+pXp4que5U+Zpn+czMvMnwF7gHwPnV5f5gM6XA5m4lrzbq9NdmZmvAP+J6ZrjtwHXRcQRmlezvQr4JAPM8UwVekS8LiJef+Y+8E+BZ1bfamq0Xh7hPcDXJpilljPFWPktpmiuq3OM9wCHMvMTLaumdp67ZZ7yeb4wIs6v7p8HvIPmuf+9NC/zAVM0z13yfr/lP/JB81z01MxxZt6WmRdn5haa37J/PDNvYoA5nqlvikbE36N5VA7Nyxb8l8z82AQjdRQRXwQaNC9/eRz4CPBfgS8BbwD+F/DPM3Nq/hGyS+YGzdMACRwB3t9yfnqiIuI3gG8AB/n/5x0/TPOc9FTO8yqZ38X0zvOv0fwHuXU0DwC/lJl3VH8Xl2ievvgu8DvV0e9ErZL3ceBCIIADwL9s+cfTqRERDeCDmXntIHM8U4UuSepupk65SJK6s9AlqRAWuiQVwkKXpEJY6JJUCAtdkgphoUtSIf4vrcttkGs0or4AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXcswEIRPvGe"
      },
      "source": [
        "max_seq_len = 25"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tk5S7DWaP2t6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c06dbbfe-99d9-4408-f226-e1393fae5eb3"
      },
      "source": [
        "# tokenize and encode sequences in the training set\n",
        "tokens_train = tokenizer.batch_encode_plus(\n",
        "    train_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "\n",
        "# tokenize and encode sequences in the validation set\n",
        "tokens_val = tokenizer.batch_encode_plus(\n",
        "    val_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "\n",
        "# tokenize and encode sequences in the test set\n",
        "tokens_test = tokenizer.batch_encode_plus(\n",
        "    test_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wsm8bkRZQTw9"
      },
      "source": [
        "# Convert Integer Sequences to Tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QR-lXwmzQPd6"
      },
      "source": [
        "# for train set\n",
        "train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "train_y = torch.tensor(train_labels.tolist())\n",
        "\n",
        "# for validation set\n",
        "val_seq = torch.tensor(tokens_val['input_ids'])\n",
        "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
        "val_y = torch.tensor(val_labels.tolist())\n",
        "\n",
        "# for test set\n",
        "test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
        "test_y = torch.tensor(test_labels.tolist())"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ov1cOBlcRLuk"
      },
      "source": [
        "# Create DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUy9JKFYQYLp"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "#define a batch size\n",
        "batch_size = 32\n",
        "\n",
        "# wrap tensors\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "train_sampler = RandomSampler(train_data)\n",
        "\n",
        "# dataLoader for train set\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# wrap tensors\n",
        "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "\n",
        "# dataLoader for validation set\n",
        "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2HZc5ZYRV28"
      },
      "source": [
        "# Freeze BERT Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHZ0MC00RQA_"
      },
      "source": [
        "# freeze all the parameters\n",
        "for param in bert.parameters():\n",
        "    param.requires_grad = False"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7ahGBUWRi3X"
      },
      "source": [
        "# Define Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3iEtGyYRd0A"
      },
      "source": [
        "class BERT_Arch(nn.Module):\n",
        "\n",
        "    def __init__(self, bert):\n",
        "      \n",
        "      super(BERT_Arch, self).__init__()\n",
        "\n",
        "      self.bert = bert \n",
        "      \n",
        "      # dropout layer\n",
        "      self.dropout = nn.Dropout(0.1)\n",
        "      \n",
        "      # relu activation function\n",
        "      self.relu =  nn.ReLU()\n",
        "\n",
        "      # dense layer 1\n",
        "      self.fc1 = nn.Linear(768,512)\n",
        "      \n",
        "      # dense layer 2 (Output layer)\n",
        "      self.fc2 = nn.Linear(512,len(df['label'].unique()))\n",
        "\n",
        "      #softmax activation function\n",
        "      self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    #define the forward pass\n",
        "    def forward(self, sent_id, mask):\n",
        "\n",
        "      #pass the inputs to the model  \n",
        "      _, cls_hs = self.bert(sent_id, attention_mask=mask)\n",
        "      \n",
        "      x = self.fc1(cls_hs)\n",
        "\n",
        "      x = self.relu(x)\n",
        "\n",
        "      x = self.dropout(x)\n",
        "\n",
        "      # output layer\n",
        "      x = self.fc2(x)\n",
        "      \n",
        "      # apply softmax activation\n",
        "      x = self.softmax(x)\n",
        "\n",
        "      return x"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBAJJVuJRliv"
      },
      "source": [
        "# pass the pre-trained BERT to our define architecture\n",
        "model = BERT_Arch(bert)\n",
        "\n",
        "# push the model to GPU\n",
        "model = model.to(device)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taXS0IilRn9J"
      },
      "source": [
        "# optimizer from hugging face transformers\n",
        "from transformers import AdamW\n",
        "\n",
        "# define the optimizer\n",
        "optimizer = AdamW(model.parameters(), lr = 1e-3)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9CDpoMQR_rK"
      },
      "source": [
        "# Find Class Weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izY5xH5eR7Ur",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02aea07d-bc27-487a-94f8-cb099790e840"
      },
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "#compute the class weights\n",
        "class_wts = compute_class_weight('balanced', classes = np.unique(train_labels), y = train_labels)\n",
        "\n",
        "print(class_wts)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.75 1.5 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1WvfY2vSGKi"
      },
      "source": [
        "# convert class weights to tensor\n",
        "weights= torch.tensor(class_wts,dtype=torch.float)\n",
        "weights = weights.to(device)\n",
        "\n",
        "# loss function\n",
        "cross_entropy  = nn.NLLLoss(weight=weights) \n",
        "\n",
        "# number of training epochs\n",
        "epochs = 10"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "My4CA0qaShLq"
      },
      "source": [
        "# Fine-Tune BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rskLk8R_SahS"
      },
      "source": [
        "# function to train the model\n",
        "def train():\n",
        "  \n",
        "  model.train()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "  # empty list to save model predictions\n",
        "  total_preds=[]\n",
        "  \n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(train_dataloader):\n",
        "    \n",
        "    # progress update after every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [r.to(device) for r in batch]\n",
        " \n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    # clear previously calculated gradients \n",
        "    model.zero_grad()        \n",
        "\n",
        "    # get model predictions for the current batch\n",
        "    preds = model(sent_id, mask)\n",
        "\n",
        "    # compute the loss between actual and predicted values\n",
        "    loss = cross_entropy(preds, labels)\n",
        "\n",
        "    # add on to the total loss\n",
        "    total_loss = total_loss + loss.item()\n",
        "\n",
        "    # backward pass to calculate the gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    # model predictions are stored on GPU. So, push it to CPU\n",
        "    preds=preds.detach().cpu().numpy()\n",
        "\n",
        "    # append the model predictions\n",
        "    total_preds.append(preds)\n",
        "\n",
        "  # compute the training loss of the epoch\n",
        "  avg_loss = total_loss / len(train_dataloader)\n",
        "  \n",
        "  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  #returns the loss and predictions\n",
        "  return avg_loss, total_preds"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGXovFDlSxB5"
      },
      "source": [
        "# function for evaluating the model\n",
        "def evaluate():\n",
        "  \n",
        "  print(\"\\nEvaluating...\")\n",
        "  \n",
        "  # deactivate dropout layers\n",
        "  model.eval()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "  # empty list to save the model predictions\n",
        "  total_preds = []\n",
        "\n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(val_dataloader):\n",
        "    \n",
        "    # Progress update every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      \n",
        "      # Calculate elapsed time in minutes.\n",
        "      elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "      # Report progress.\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [t.to(device) for t in batch]\n",
        "\n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    # deactivate autograd\n",
        "    with torch.no_grad():\n",
        "      \n",
        "      # model predictions\n",
        "      preds = model(sent_id, mask)\n",
        "\n",
        "      # compute the validation loss between actual and predicted values\n",
        "      loss = cross_entropy(preds,labels)\n",
        "\n",
        "      total_loss = total_loss + loss.item()\n",
        "\n",
        "      preds = preds.detach().cpu().numpy()\n",
        "\n",
        "      total_preds.append(preds)\n",
        "\n",
        "  # compute the validation loss of the epoch\n",
        "  avg_loss = total_loss / len(val_dataloader) \n",
        "\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  return avg_loss, total_preds"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KZEgxRRTLXG"
      },
      "source": [
        "# Start Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1USGTntS3TS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f66b4a8-19e9-4670-93cb-451c53d52bf0"
      },
      "source": [
        "# set initial loss to infinite\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "# empty lists to store training and validation loss of each epoch\n",
        "train_losses=[]\n",
        "valid_losses=[]\n",
        "\n",
        "#for each epoch\n",
        "for epoch in range(epochs):\n",
        "     \n",
        "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "    \n",
        "    #train model\n",
        "    train_loss, _ = train()\n",
        "    \n",
        "    #evaluate model\n",
        "    valid_loss, _ = evaluate()\n",
        "    \n",
        "    #save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    \n",
        "    # append training and validation loss\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "    \n",
        "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
        "    print(f'Validation Loss: {valid_loss:.3f}')"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1 / 10\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.700\n",
            "Validation Loss: 0.700\n",
            "\n",
            " Epoch 2 / 10\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.696\n",
            "Validation Loss: 0.690\n",
            "\n",
            " Epoch 3 / 10\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.694\n",
            "Validation Loss: 0.686\n",
            "\n",
            " Epoch 4 / 10\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.676\n",
            "Validation Loss: 0.683\n",
            "\n",
            " Epoch 5 / 10\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.653\n",
            "Validation Loss: 0.681\n",
            "\n",
            " Epoch 6 / 10\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.639\n",
            "Validation Loss: 0.683\n",
            "\n",
            " Epoch 7 / 10\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.647\n",
            "Validation Loss: 0.680\n",
            "\n",
            " Epoch 8 / 10\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.617\n",
            "Validation Loss: 0.671\n",
            "\n",
            " Epoch 9 / 10\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.648\n",
            "Validation Loss: 0.667\n",
            "\n",
            " Epoch 10 / 10\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.623\n",
            "Validation Loss: 0.670\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yrhUc9kTI5a"
      },
      "source": [
        "# Load Saved Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OacxUyizS8d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29aa689f-d8bf-4770-c181-3293098235df"
      },
      "source": [
        "#load weights of best model\n",
        "path = 'saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4SVftkkTZXA"
      },
      "source": [
        "# Get Predictions for Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZl0SZmFTRQA"
      },
      "source": [
        "# get predictions for test data\n",
        "with torch.no_grad():\n",
        "  preds = model(test_seq.to(device), test_mask.to(device))\n",
        "  preds = preds.detach().cpu().numpy()"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ms1ObHZxTYSI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad64d237-65a6-45a7-ce34-08e21030dc19"
      },
      "source": [
        "# model's performance\n",
        "preds = np.argmax(preds, axis = 1)\n",
        "print(classification_report(test_y, preds))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.95      0.86        19\n",
            "           1       0.80      0.44      0.57         9\n",
            "\n",
            "    accuracy                           0.79        28\n",
            "   macro avg       0.79      0.70      0.71        28\n",
            "weighted avg       0.79      0.79      0.77        28\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqzLS7rHTp4T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "631b1d76-a039-4f30-8dc2-cbe08e49fac5"
      },
      "source": [
        "# confusion matrix\n",
        "pd.crosstab(test_y, preds)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>col_0</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>row_0</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>18</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "col_0   0  1\n",
              "row_0       \n",
              "0      18  1\n",
              "1       5  4"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    }
  ]
}